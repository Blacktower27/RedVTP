# RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models

This repository serves as the main project page for **RedVTP**, a training-free visual token pruning method for Diffusion Vision-Language Models (DVLMs).  
RedVTP improves inference efficiency by pruning redundant vision tokens without requiring any training or finetuning.

---

## Repositories

RedVTP is implemented on top of two DVLM baselines.  
Please visit the corresponding repositories below for installation and inference instructions.

- **RedVTP-LLaDA-V**  
  https://github.com/Blacktower27/RedVTP-LLaDA-V

- **RedVTP-LaViDa**  
  https://github.com/Blacktower27/RedVTP-LaViDa

Each repository contains the minimal modified baseline code and the pruning-based inference scripts.

---

## Citation

If you find this work useful, please cite:

```bibtex
@article{xu2025redvtp,
  title={RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models},
  author={Xu, Jingqi and Lu, Jingxi and Li, Chenghao and Sarkar, Sreetama and Kundu, Souvik and Beerel, Peter},
  journal={arXiv preprint arXiv:2505.xxxxx},
  year={2025}
}
